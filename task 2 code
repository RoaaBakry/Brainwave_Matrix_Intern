#Sentiment Analysis on Amazon Fine Food Reviews using Bidirectional LSTM

#This script loads the Amazon Fine Food Reviews dataset and performs multi-class sentiment classification 
#using a deep learning model. It includes preprocessing steps such as lowercasing, punctuation removal, 
#stopword removal, and lemmatization to clean the text data. Reviews are labeled as Negative (0), Neutral (1), 
#or Positive (2) based on the original review score.

#Text data is tokenized and padded before being fed into a Bidirectional LSTM model. The model is trained 
#to predict sentiment labels and evaluated using classification metrics and confusion matrix visualization. 
#Finally, a custom function allows for real-time sentiment prediction on new reviews.

#Libraries used: pandas, numpy, nltk, sklearn, TensorFlow (Keras), matplotlib, seabornimport numpy as np 
import pandas as pd
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
import nltk

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset
df = pd.read_csv('../input/amazon-fine-food-reviews/Reviews.csv')

# Preprocessing functions
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

# Preprocess the text data
df['Text'] = df['Text'].apply(preprocess_text)

# Handle score distribution imbalance
score_counts = df['Score'].value_counts()
plt.figure(figsize=(10,6))
sns.barplot(x=score_counts.index, y=score_counts.values, palette="viridis")
plt.title('Score Distribution')
plt.xlabel('Score')
plt.ylabel('Count')
plt.show()

# Convert scores to sentiment classes
def score_to_sentiment(score):
    if score <= 2: 
        return 0  # Negative
    elif score == 3:
        return 1  # Neutral
    else: 
        return 2  # Positive

df['Sentiment'] = df['Score'].apply(score_to_sentiment)

# Split the dataset
X = df['Text']
y = to_categorical(df['Sentiment'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=df['Sentiment']
)

# Tokenize text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences
max_length = 200
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# Build LSTM model
def create_model():
    model = Sequential()
    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, 
                        output_dim=128, 
                        input_length=max_length))
    model.add(SpatialDropout1D(0.3))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy',
                 optimizer='adam',
                 metrics=['accuracy'])
    return model

model = create_model()
model.summary()

# Train the model
checkpoint = ModelCheckpoint(
    'best_model.h5', 
    monitor='val_accuracy', 
    verbose=1, 
    save_best_only=True, 
    mode='max'
)

history = model.fit(
    X_train_pad,
    y_train,
    epochs=10,
    batch_size=512,
    validation_split=0.1,
    callbacks=[checkpoint],
    verbose=1
)

# Evaluate the model
best_model = load_model('best_model.h5')
test_loss, test_acc = best_model.evaluate(X_test_pad, y_test, verbose=0)
print(f'\nTest Accuracy: {test_acc:.4f}')
print(f'Test Loss: {test_loss:.4f}')

# Generate predictions
y_pred = best_model.predict(X_test_pad)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred_classes, 
                            target_names=['Negative', 'Neutral', 'Positive']))

# Confusion matrix
def plot_confusion_matrix(cm, classes):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

cm = confusion_matrix(y_true, y_pred_classes)
plot_confusion_matrix(cm, ['Negative', 'Neutral', 'Positive'])

# Plot training history
def plot_history(history):
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

plot_history(history)

# Sentiment prediction function
def predict_sentiment(text):
    # Preprocess input text
    text = preprocess_text(text)
    # Tokenize and pad
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_length, padding='post')
    # Predict
    pred = best_model.predict(padded)
    # Get sentiment label
    sentiment = np.argmax(pred)
    sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
    confidence = np.max(pred)
    
    print(f"Text: {text}")
    print(f"Predicted Sentiment: {sentiment_labels[sentiment]} "
          f"(Confidence: {confidence:.2f})")
    print(f"Score Breakdown: [Negative: {pred[0][0]:.4f}, "
          f"Neutral: {pred[0][1]:.4f}, Positive: {pred[0][2]:.4f}]")
    print("-" * 50)

# Test with sample reviews
sample_reviews = [
    "This product was a complete waste of money. It broke after two days of use!",
    "The item arrived on time and was exactly as described in the product details.",
    "It's okay for the price but I expected better quality. Does the job though.",
    "Absolutely love this product! Exceeded all my expectations. Worth every penny!",
    "The food arrived stale and expired. Very disappointed with this purchase."
]

for review in sample_reviews:
    predict_sentiment(review)
